{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d386fd3e-7789-4d88-8655-a67b39e1e4cd",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "1. [Import Libraries and Load Raw Data](#Import-Libraries-and-Load-Raw-Data)\n",
    "2. [Date and Time Processing](#Date-and-Time-Processing)\n",
    "3. [Remove Missing Data](#Remove-Missing-Data)\n",
    "4. [Date Range Filtering](#Date-Range-Filtering)\n",
    "5. [Add Stock Data](#Add-Stock-Data)\n",
    "6. [Drop Duplicates](#Drop-Duplicates)\n",
    "7. [Filter out Uninformative Headlines](#Filter-out-Uninformative-Headlines)\n",
    "8. [Remove Similar Headlines](#Remove-Similar-Headlines)\n",
    "9. [Filter for Valid Trading Sessions after Headline](#Filter-for-Valid-Trading-Sessions-after-Headline)\n",
    "10. [Calculate Trading Returns](#Calculate-Trading-Returns)\n",
    "11. [Stratified Sampling](#Stratified-Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae5fe3-c062-460d-8a99-1c198b7c644a",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3017f7e6-a06a-499b-ac99-f059b958c111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stocks That Hit 52-Week Highs On Friday</td>\n",
       "      <td>2020-06-05 10:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stocks That Hit 52-Week Highs On Wednesday</td>\n",
       "      <td>2020-06-03 10:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71 Biggest Movers From Friday</td>\n",
       "      <td>2020-05-26 04:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46 Stocks Moving In Friday's Mid-Day Session</td>\n",
       "      <td>2020-05-22 12:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B of A Securities Maintains Neutral on Agilent...</td>\n",
       "      <td>2020-05-22 11:38:00-04:00</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0            Stocks That Hit 52-Week Highs On Friday   \n",
       "1         Stocks That Hit 52-Week Highs On Wednesday   \n",
       "2                      71 Biggest Movers From Friday   \n",
       "3       46 Stocks Moving In Friday's Mid-Day Session   \n",
       "4  B of A Securities Maintains Neutral on Agilent...   \n",
       "\n",
       "                        date stock  \n",
       "0  2020-06-05 10:30:00-04:00     A  \n",
       "1  2020-06-03 10:45:00-04:00     A  \n",
       "2  2020-05-26 04:30:00-04:00     A  \n",
       "3  2020-05-22 12:45:00-04:00     A  \n",
       "4  2020-05-22 11:38:00-04:00     A  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import yfinance as yf\n",
    "#from pandas_datareader import data\n",
    "from typing import Optional\n",
    "import re\n",
    "import string\n",
    "from textdistance import damerau_levenshtein\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_path = \"source/analyst_ratings_processed.csv\"  \n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "df = df[['title', 'date', 'stock']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27ce759-eeee-4850-bc1a-256e847509a9",
   "metadata": {},
   "source": [
    "## Date and Time Processing\n",
    "Restructure the datetime information, splitting into separate date, time and offset columns for easier processing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3e0666-02d9-48be-9bc4-6c39f7e4d915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>datetime</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stocks That Hit 52-Week Highs On Friday</td>\n",
       "      <td>2020-06-05 10:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>2020-06-05</td>\n",
       "      <td>10:30:00</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stocks That Hit 52-Week Highs On Wednesday</td>\n",
       "      <td>2020-06-03 10:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>2020-06-03</td>\n",
       "      <td>10:45:00</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71 Biggest Movers From Friday</td>\n",
       "      <td>2020-05-26 04:30:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>2020-05-26</td>\n",
       "      <td>04:30:00</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46 Stocks Moving In Friday's Mid-Day Session</td>\n",
       "      <td>2020-05-22 12:45:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>12:45:00</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B of A Securities Maintains Neutral on Agilent...</td>\n",
       "      <td>2020-05-22 11:38:00-04:00</td>\n",
       "      <td>A</td>\n",
       "      <td>2020-05-22</td>\n",
       "      <td>11:38:00</td>\n",
       "      <td>04:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0            Stocks That Hit 52-Week Highs On Friday   \n",
       "1         Stocks That Hit 52-Week Highs On Wednesday   \n",
       "2                      71 Biggest Movers From Friday   \n",
       "3       46 Stocks Moving In Friday's Mid-Day Session   \n",
       "4  B of A Securities Maintains Neutral on Agilent...   \n",
       "\n",
       "                    datetime stock        date      time offset  \n",
       "0  2020-06-05 10:30:00-04:00     A  2020-06-05  10:30:00  04:00  \n",
       "1  2020-06-03 10:45:00-04:00     A  2020-06-03  10:45:00  04:00  \n",
       "2  2020-05-26 04:30:00-04:00     A  2020-05-26  04:30:00  04:00  \n",
       "3  2020-05-22 12:45:00-04:00     A  2020-05-22  12:45:00  04:00  \n",
       "4  2020-05-22 11:38:00-04:00     A  2020-05-22  11:38:00  04:00  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rename(columns={'date': 'datetime'}, inplace=True)\n",
    "\n",
    "df['date'] = df['datetime']\n",
    "df['time'] = df['date'].str.split(' ').str[1]\n",
    "df['date'] = df['date'].str.split(' ').str[0]\n",
    "df['offset'] = df['time'].str.split('-').str[1]\n",
    "df['time'] = df['time'].str.split('-').str[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717cd45-5cee-40ec-8a5d-c5dae236497e",
   "metadata": {},
   "source": [
    "## Remove Missing Data\n",
    "Filter out rows with missing datetime values. Datetime objects with midnight timestamps (00:00:00) are also removed as this implies that no time data was available and the default value was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a68177e-0efa-429b-bbfa-40e798110bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values with no time to be removed: 640\n",
      "Null datetime values to be removed: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Values with no time to be removed: {len(df[df['time'] == '00:00:00'])}\")\n",
    "print(f\"Null datetime values to be removed: {len(df[df['datetime'].isna()])}\")\n",
    "\n",
    "df = df[df['time'] != '00:00:00']\n",
    "df = df[~df['datetime'].isna()]\n",
    "\n",
    "df = df[['title', 'datetime', 'stock', 'date', 'time', 'offset']]\n",
    "df.to_csv('output2/filtered_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b9d1b-ad10-4788-aca6-aeba9d25c2c0",
   "metadata": {},
   "source": [
    "## Date Range Filtering\n",
    "Remove all headlines outside the research period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271d4d3f-9dd2-45fa-a434-d6e04c40cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original headlines: 1398540\n",
      "Filtered headlines: 191689\n"
     ]
    }
   ],
   "source": [
    "def filter_stock_data(input_path, output_path):\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Create datetime objects for comparison\n",
    "        start_date = pd.to_datetime('2018-10-01')\n",
    "        end_date = pd.to_datetime('2019-12-31')\n",
    "        \n",
    "        # Filter data between the specified dates\n",
    "        mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "        filtered_df = df.loc[mask]\n",
    "        \n",
    "        # Save filtered data \n",
    "        filtered_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"Original headlines: {len(df)}\")\n",
    "        print(f\"Filtered headlines: {len(filtered_df)}\")\n",
    "        \n",
    "        return filtered_df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: CSV file not found\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "input_path = \"output2/filtered_data.csv\"  \n",
    "output_path = \"output2/filtered_data2.csv\" \n",
    "filtered_data = filter_stock_data(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26a2be1-2b84-4855-8045-6f95a48a4799",
   "metadata": {},
   "source": [
    "## Add Stock Data\n",
    "This section enriches the dataset by adding: \n",
    "\n",
    "- Exchange\n",
    "- Share code\n",
    "- Company name\n",
    "\n",
    "to each row. Afterwards stocks are filtered to only those with share codes 10/11.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c15225a-df13-4376-afbc-8dc6a0516812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique stocks in input: 4141\n",
      "Most common stocks in input:\n",
      "stock\n",
      "PCG      1034\n",
      "NFLX      978\n",
      "GOOGL     941\n",
      "EWU       907\n",
      "DIA       838\n",
      "Name: count, dtype: int64\n",
      "Downloading exchange data...\n",
      "Fetching NASDAQ data...\n",
      "Found 11257 records from NASDAQ\n",
      "Sample symbols and share codes:\n",
      "  Symbol                                   Security_Name  Share_Code\n",
      "0      A         Agilent Technologies, Inc. Common Stock          10\n",
      "1     AA                 Alcoa Corporation Common Stock           10\n",
      "2    AAA  Alternative Access First Priority CLO Bond ETF          10\n",
      "3   AAAU          Goldman Sachs Physical Gold ETF Shares          10\n",
      "4  AACBU              Artius II Acquisition Inc. - Units          11\n",
      "Fetching NYSE/AMEX data...\n",
      "Found 6437 records from NYSE/AMEX\n",
      "Sample symbols and share codes:\n",
      "  Symbol                                      Security_Name  Share_Code\n",
      "0      A            Agilent Technologies, Inc. Common Stock          10\n",
      "1     AA                    Alcoa Corporation Common Stock           10\n",
      "2    AAA     Alternative Access First Priority CLO Bond ETF          10\n",
      "3   AAAU             Goldman Sachs Physical Gold ETF Shares          10\n",
      "4   AACT  Ares Acquisition Corporation II Class A Ordina...          10\n",
      "\n",
      "Total valid records after filtering: 7167\n",
      "\n",
      "Exchange distribution:\n",
      "Exchange\n",
      "NASDAQ    4011\n",
      "NYSE      2846\n",
      "AMEX       310\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Share code distribution:\n",
      "Share_Code\n",
      "10    6359\n",
      "11     808\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of ticker mapping (Symbol: Exchange, Share Code, Company):\n",
      "AACBU: NASDAQ, 11, Artius II Acquisition Inc. - Units\n",
      "AACG: NASDAQ, 11, ATA Creativity Global - American Depositary Shares, each representing two common shares\n",
      "AADI: NASDAQ, 10, Aadi Bioscience, Inc. - Common Stock\n",
      "AAL: NASDAQ, 10, American Airlines Group, Inc. - Common Stock\n",
      "AAME: NASDAQ, 10, Atlantic American Corporation - Common Stock\n",
      "\n",
      "Filtering results:\n",
      "Original rows: 191689\n",
      "Filtered rows: 129759\n",
      "\n",
      "Matches by exchange:\n",
      "exchange\n",
      "NYSE      73124\n",
      "NASDAQ    54536\n",
      "AMEX       2099\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Matches by share code:\n",
      "share_code\n",
      "10    121025\n",
      "11      8734\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Results saved to: output2/filtered_data3.csv\n",
      "\n",
      "Sample of processed data:\n",
      "                                               title        date stock  \\\n",
      "0            Stocks That Hit 52-Week Highs On Friday  2019-12-27     A   \n",
      "1            Stocks That Hit 52-Week Highs On Monday  2019-12-23     A   \n",
      "2            Stocks That Hit 52-Week Highs On Friday  2019-12-20     A   \n",
      "3          Stocks That Hit 52-Week Highs On Thursday  2019-12-12     A   \n",
      "4  There's A New Trading Tool That Allows Traders...  2019-12-11     A   \n",
      "\n",
      "  exchange share_code          company_name  \n",
      "0     NYSE         10  Agilent Technologies  \n",
      "1     NYSE         10  Agilent Technologies  \n",
      "2     NYSE         10  Agilent Technologies  \n",
      "3     NYSE         10  Agilent Technologies  \n",
      "4     NYSE         10  Agilent Technologies  \n",
      "\n",
      "Top 10 stocks in filtered data:\n",
      "stock\n",
      "PCG      1034\n",
      "NFLX      978\n",
      "GOOGL     941\n",
      "GOOG      759\n",
      "TSLA      733\n",
      "NVDA      642\n",
      "MU        637\n",
      "FDX       594\n",
      "BABA      545\n",
      "JNJ       539\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def determine_share_code(security_name):\n",
    "    if pd.isna(security_name):\n",
    "        return 10\n",
    "        \n",
    "    if not isinstance(security_name, str):\n",
    "        security_name = str(security_name)\n",
    "        \n",
    "    unit_indicators = [\n",
    "        ' UNIT',\n",
    "        ' UNITS',\n",
    "        'UNIT ',\n",
    "        'UNITS ',\n",
    "        ' AND ',\n",
    "        '/WRNTS',\n",
    "        '/WRNT',\n",
    "        '/RTS',\n",
    "        'RIGHTS',\n",
    "        'PAIRED SHARE',\n",
    "        ' PER SHARE',\n",
    "        ' - UNIT',\n",
    "        ' - UNITS',\n",
    "        'DEPOSITARY SHARE'\n",
    "    ]\n",
    "    \n",
    "    if any(indicator in security_name.upper() for indicator in unit_indicators):\n",
    "        return 11\n",
    "    return 10\n",
    "\n",
    "def download_exchange_data():\n",
    "    \"\"\"\n",
    "    Downloads current ticker data from NASDAQ FTP site\n",
    "    \"\"\"\n",
    "    print(\"Downloading exchange data...\")\n",
    "    \n",
    "    urls = {\n",
    "        'NASDAQ': 'https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt',\n",
    "        'NYSE/AMEX': 'https://www.nasdaqtrader.com/dynamic/SymDir/otherlisted.txt'\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for exchange_name, url in urls.items():\n",
    "        try:\n",
    "            print(f\"Fetching {exchange_name} data...\")\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                # Read the data using pandas directly\n",
    "                df = pd.read_csv(io.StringIO(response.text), delimiter='|', comment='#')\n",
    "                \n",
    "                # Ensure Symbol column is string type\n",
    "                if 'Symbol' in df.columns:\n",
    "                    df['Symbol'] = df['Symbol'].astype(str)\n",
    "                if 'ACT Symbol' in df.columns:\n",
    "                    df['ACT Symbol'] = df['ACT Symbol'].astype(str)\n",
    "                \n",
    "                # For NASDAQ file\n",
    "                if 'nasdaqtraded.txt' in url:\n",
    "                    df = df.rename(columns={\n",
    "                        'Symbol': 'Symbol',\n",
    "                        'Security Name': 'Security_Name',\n",
    "                        'ETF': 'ETF',\n",
    "                        'Round Lot Size': 'Round_Lot_Size',\n",
    "                        'Test Issue': 'Test_Issue'\n",
    "                    })\n",
    "                    # NASDAQ file needs exchange set explicitly\n",
    "                    df['Exchange'] = df['Market Category'].apply(lambda x: 'NASDAQ' if x in ['Q', 'G', 'S'] else None)\n",
    "                \n",
    "                # For NYSE/AMEX file\n",
    "                else:\n",
    "                    df = df.rename(columns={\n",
    "                        'ACT Symbol': 'Symbol',\n",
    "                        'Security Name': 'Security_Name',\n",
    "                        'Exchange': 'Exchange',\n",
    "                        'ETF': 'ETF',\n",
    "                        'Round Lot Size': 'Round_Lot_Size',\n",
    "                        'Test Issue': 'Test_Issue'\n",
    "                    })\n",
    "                \n",
    "                # Add share code based on security name\n",
    "                df['Share_Code'] = df['Security_Name'].apply(determine_share_code)\n",
    "                \n",
    "                print(f\"Found {len(df)} records from {exchange_name}\")\n",
    "                print(\"Sample symbols and share codes:\")\n",
    "                print(df[['Symbol', 'Security_Name', 'Share_Code']].head())\n",
    "                \n",
    "                all_data.append(df)\n",
    "            \n",
    "            time.sleep(1)  # Be nice to the server\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {exchange_name} data: {str(e)}\")\n",
    "            if 'response' in locals():\n",
    "                print(\"Response status:\", response.status_code)\n",
    "                print(\"Response headers:\", response.headers)\n",
    "                print(\"First 500 chars of response:\", response.text[:500])\n",
    "    \n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        # Clean up the data\n",
    "        combined_df = combined_df[\n",
    "            (combined_df['Test_Issue'] == 'N') &  # Remove test issues\n",
    "            (combined_df['ETF'] == 'N')  # Remove ETFs\n",
    "        ]\n",
    "        \n",
    "        # Map exchange codes to names\n",
    "        exchange_map = {\n",
    "            'N': 'NYSE',\n",
    "            'A': 'AMEX',\n",
    "            'Q': 'NASDAQ',\n",
    "            'P': 'NYSE',  # NYSE Arca\n",
    "            'Z': 'NYSE',  # NYSE BATS\n",
    "            'V': 'NYSE',  # NYSE IEX\n",
    "            'NASDAQ': 'NASDAQ',  # Direct NASDAQ listing\n",
    "            'NYSE': 'NYSE',      # Direct NYSE listing\n",
    "            'AMEX': 'AMEX'       # Direct AMEX listing\n",
    "        }\n",
    "        \n",
    "        combined_df['Exchange'] = combined_df['Exchange'].map(exchange_map)\n",
    "        combined_df = combined_df[combined_df['Exchange'].notna()]\n",
    "        \n",
    "        print(f\"\\nTotal valid records after filtering: {len(combined_df)}\")\n",
    "        print(\"\\nExchange distribution:\")\n",
    "        print(combined_df['Exchange'].value_counts())\n",
    "        print(\"\\nShare code distribution:\")\n",
    "        print(combined_df['Share_Code'].value_counts())\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def clean_company_names(df):\n",
    "    terms_to_remove = [\n",
    "        \"Inc\", \"Common\", \"Stock\", \"Equity\", \"Registry\", \"Shares\", \n",
    "        \"Limited\", \"Corp\", \"Corporation\", \"Holdings\", \"Industries\", \n",
    "        \"Depository\", \"Ordinary\", \"Class B\", \"Class A\", \"Class C\"\n",
    "    ]\n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Function to clean each company name\n",
    "    def clean_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return name\n",
    "            \n",
    "        # Remove each term (with word boundaries)\n",
    "        cleaned_name = name\n",
    "        for term in terms_to_remove:\n",
    "            pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "            cleaned_name = re.sub(pattern, '', cleaned_name)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        cleaned_name = cleaned_name.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remove any double spaces created and trim\n",
    "        cleaned_name = \" \".join(cleaned_name.split())\n",
    "        \n",
    "        return cleaned_name.strip()\n",
    "    \n",
    "    # Apply the cleaning function\n",
    "    df_clean['company_name'] = df_clean['company_name'].apply(clean_name)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def process_news_data(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Process news CSV file and filter based on exchange listings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the input CSV\n",
    "        df = pd.read_csv(input_file)\n",
    "        \n",
    "        # Ensure stock column is string type\n",
    "        df['stock'] = df['stock'].astype(str)\n",
    "        \n",
    "        # Print unique stocks for debugging\n",
    "        unique_stocks = df['stock'].nunique()\n",
    "        print(f\"\\nUnique stocks in input: {unique_stocks}\")\n",
    "        print(\"Most common stocks in input:\")\n",
    "        print(df['stock'].value_counts().head())\n",
    "        \n",
    "        # Get exchange data\n",
    "        exchange_df = download_exchange_data()\n",
    "        \n",
    "        if exchange_df.empty:\n",
    "            raise Exception(\"Could not obtain exchange data\")\n",
    "        \n",
    "        # Create ticker to info mapping (exchange, share code, and company name)\n",
    "        # Ensure all symbols are strings and uppercase\n",
    "        ticker_info_map = dict(zip(\n",
    "            exchange_df['Symbol'].astype(str).str.strip().str.upper(),\n",
    "            zip(exchange_df['Exchange'], exchange_df['Share_Code'], exchange_df['Security_Name'])\n",
    "        ))\n",
    "        \n",
    "        print(\"\\nSample of ticker mapping (Symbol: Exchange, Share Code, Company):\")\n",
    "        sample_tickers = list(ticker_info_map.items())[:5]\n",
    "        for ticker, (exchange, share_code, company_name) in sample_tickers:\n",
    "            print(f\"{ticker}: {exchange}, {share_code}, {company_name}\")\n",
    "        \n",
    "        # Process news data\n",
    "        df['stock_upper'] = df['stock'].astype(str).str.strip().str.upper()\n",
    "        df['exchange'] = None\n",
    "        df['share_code'] = None\n",
    "        df['company_name'] = None\n",
    "               \n",
    "        # Apply exchange, share code, and company name information\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['stock_upper'] in ticker_info_map:\n",
    "                exchange, share_code, company_name = ticker_info_map[row['stock_upper']]\n",
    "                df.at[idx, 'exchange'] = exchange\n",
    "                df.at[idx, 'share_code'] = share_code\n",
    "                df.at[idx, 'company_name'] = company_name\n",
    "        \n",
    "        # Remove temporary column\n",
    "        df = df.drop('stock_upper', axis=1)\n",
    "        \n",
    "        # Filter and sort\n",
    "        df_filtered = df[df['exchange'].notna()].copy()\n",
    "        \n",
    "        print(f\"\\nFiltering results:\")\n",
    "        print(f\"Original rows: {len(df)}\")\n",
    "        print(f\"Filtered rows: {len(df_filtered)}\")\n",
    "        print(f\"\\nMatches by exchange:\")\n",
    "        print(df_filtered['exchange'].value_counts())\n",
    "        print(f\"\\nMatches by share code:\")\n",
    "        print(df_filtered['share_code'].value_counts())\n",
    "\n",
    "        # Clean company names\n",
    "        df_cleaned = clean_company_names(df_filtered)\n",
    "        \n",
    "        # Save results\n",
    "        df_cleaned.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "        if not df_cleaned.empty:\n",
    "            print(\"\\nSample of processed data:\")\n",
    "            print(df_cleaned[['title', 'date', 'stock', 'exchange', 'share_code', 'company_name']].head())\n",
    "            \n",
    "            # Print some statistics\n",
    "            print(\"\\nTop 10 stocks in filtered data:\")\n",
    "            print(df_cleaned['stock'].value_counts().head(10))\n",
    "        else:\n",
    "            print(\"\\nNo matches found in the data\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "input_file = \"output2/filtered_data2.csv\"  \n",
    "output_file = \"output2/filtered_data3.csv\"   \n",
    "process_news_data(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1eb67-170f-4827-b99c-84471b40d55d",
   "metadata": {},
   "source": [
    "## Drop Duplicates\n",
    "Remove duplicates based on title, date, and stock to prevent bias from repeated headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77ce193-07ed-42f3-a8c6-97eac4bdf218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial row count: 129759\n",
      "Updated row count: 129699\n",
      "Duplicates removed: 60\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(input_path, output_path):\n",
    "\n",
    "    df = pd.read_csv(input_path)  \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    df_deduped = df.drop_duplicates(subset=['title', 'date', 'stock'], keep='first')\n",
    "    final_count = len(df_deduped)\n",
    "    \n",
    "    df_deduped.to_csv(output_path, index=False)\n",
    "    \n",
    "    return {\n",
    "        'initial_count': initial_count,\n",
    "        'final_count': final_count,\n",
    "        'duplicates_removed': initial_count - final_count\n",
    "    }\n",
    "\n",
    "\n",
    "input_file = \"output2/filtered_data3.csv\"\n",
    "output_file = \"output2/filtered_data4.csv\"\n",
    "\n",
    "results = remove_duplicates(input_file, output_file)\n",
    "print(f\"Initial row count: {results['initial_count']}\")\n",
    "print(f\"Updated row count: {results['final_count']}\")\n",
    "print(f\"Duplicates removed: {results['duplicates_removed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406a4a02-535c-485c-b43f-c70cb943d87e",
   "metadata": {},
   "source": [
    "## Drop Low Relevance Headlines\n",
    "For our purposes, a headline is considered low relevance if it doesn't either contain the name of the stock ticker or the company name in the headline. As part of this, stock tickers with only 1 letter will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f87784e-a044-43c3-af05-59f667f94b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 129699\n",
      "Dropped rows: 65652\n",
      "Reminaing rows: 64047\n",
      "Percentage kept: 49.38%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>datetime</th>\n",
       "      <th>stock</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>offset</th>\n",
       "      <th>exchange</th>\n",
       "      <th>share_code</th>\n",
       "      <th>company_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Atlantic American Q3 EPS $(0.07) Down From $0....</td>\n",
       "      <td>2019-11-12 14:11:00-05:00</td>\n",
       "      <td>AAME</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>14:11:00</td>\n",
       "      <td>05:00</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>10</td>\n",
       "      <td>Atlantic American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Atlantic American Q1 EPS $0.19 Up From $(0.25)...</td>\n",
       "      <td>2019-05-13 10:04:00-04:00</td>\n",
       "      <td>AAME</td>\n",
       "      <td>2019-05-13</td>\n",
       "      <td>10:04:00</td>\n",
       "      <td>04:00</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>10</td>\n",
       "      <td>Atlantic American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Atlantic American Q4 EPS $0.01 Down From $0.12...</td>\n",
       "      <td>2019-04-01 14:04:00-04:00</td>\n",
       "      <td>AAME</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>14:04:00</td>\n",
       "      <td>04:00</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>10</td>\n",
       "      <td>Atlantic American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Atlantic American Q3 EPS $0.04 Up From $0.03 Y...</td>\n",
       "      <td>2018-11-13 12:14:00-05:00</td>\n",
       "      <td>AAME</td>\n",
       "      <td>2018-11-13</td>\n",
       "      <td>12:14:00</td>\n",
       "      <td>05:00</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>10</td>\n",
       "      <td>Atlantic American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Applied Optoelectronics to Participate in Raym...</td>\n",
       "      <td>2019-12-03 16:11:00-05:00</td>\n",
       "      <td>AAOI</td>\n",
       "      <td>2019-12-03</td>\n",
       "      <td>16:11:00</td>\n",
       "      <td>05:00</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>10</td>\n",
       "      <td>Applied Optoelectronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "103  Atlantic American Q3 EPS $(0.07) Down From $0....   \n",
       "107  Atlantic American Q1 EPS $0.19 Up From $(0.25)...   \n",
       "108  Atlantic American Q4 EPS $0.01 Down From $0.12...   \n",
       "109  Atlantic American Q3 EPS $0.04 Up From $0.03 Y...   \n",
       "111  Applied Optoelectronics to Participate in Raym...   \n",
       "\n",
       "                      datetime stock        date      time offset exchange  \\\n",
       "103  2019-11-12 14:11:00-05:00  AAME  2019-11-12  14:11:00  05:00   NASDAQ   \n",
       "107  2019-05-13 10:04:00-04:00  AAME  2019-05-13  10:04:00  04:00   NASDAQ   \n",
       "108  2019-04-01 14:04:00-04:00  AAME  2019-04-01  14:04:00  04:00   NASDAQ   \n",
       "109  2018-11-13 12:14:00-05:00  AAME  2018-11-13  12:14:00  05:00   NASDAQ   \n",
       "111  2019-12-03 16:11:00-05:00  AAOI  2019-12-03  16:11:00  05:00   NASDAQ   \n",
       "\n",
       "     share_code             company_name  \n",
       "103          10        Atlantic American  \n",
       "107          10        Atlantic American  \n",
       "108          10        Atlantic American  \n",
       "109          10        Atlantic American  \n",
       "111          10  Applied Optoelectronics  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_titles_by_relevance(input_path, output_path):\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Keep track of original row count\n",
    "    original_count = len(df)\n",
    "\n",
    "    # Drop stock tickers with only a single character\n",
    "    mask = df['stock'].astype(str).str.len() > 1\n",
    "    df = df[mask]\n",
    "    \n",
    "    def contains_relevant_words(row):\n",
    "        if pd.isna(row['title']):\n",
    "            return False\n",
    "            \n",
    "        title = str(row['title']).lower()\n",
    "        \n",
    "        # Get words from stock and company_name\n",
    "        stock_words = []\n",
    "        if not pd.isna(row['stock']):\n",
    "            stock_words = [word.lower() for word in re.split(r'\\s+', str(row['stock'])) if word]\n",
    "            \n",
    "        company_words = []\n",
    "        if not pd.isna(row['company_name']):\n",
    "            company_words = [word.lower() for word in re.split(r'\\s+', str(row['company_name'])) if len(word) > 1]\n",
    "            \n",
    "        all_words = stock_words + company_words\n",
    "        \n",
    "        # Check if any word from stock or company_name appears in the title\n",
    "        return any(word in title for word in all_words if word)\n",
    "    \n",
    "    # Apply the filter\n",
    "    filtered_df = df[df.apply(contains_relevant_words, axis=1)]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    filtered_count = len(filtered_df)\n",
    "    dropped_count = original_count - filtered_count\n",
    "    \n",
    "    stats = {\n",
    "        'original_rows': original_count,\n",
    "        'filtered_rows': filtered_count,\n",
    "        'dropped_rows': dropped_count,\n",
    "        'percentage_kept': round(filtered_count / original_count * 100, 2) if original_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "        \n",
    "    return filtered_df, stats\n",
    "\n",
    "input_path = \"output2/filtered_data4.csv\"\n",
    "output_path = \"output2/filtered_data5.csv\"\n",
    "\n",
    "filtered_data, stats = filter_titles_by_relevance(input_path, output_path)\n",
    "\n",
    "print(f\"Original rows: {stats['original_rows']}\")\n",
    "print(f\"Dropped rows: {stats['dropped_rows']}\")\n",
    "print(f\"Reminaing rows: {stats['filtered_rows']}\")\n",
    "print(f\"Percentage kept: {stats['percentage_kept']}%\")\n",
    "\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c0e23-7342-49e3-9329-3129ec903e00",
   "metadata": {},
   "source": [
    "## Drop Uninformative Headlines\n",
    "Filter out uninformative headlines (like \"10 Stocks to Watch\" or \"Stocks That Hit 52-Week Highs\") that don't provide any meaningful sentiment for a stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daca1b92-fa93-4a7d-af57-81fe25426d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample classifications:\n",
      "\n",
      "\n",
      "Headline: 74 Biggest Movers From Yesterday\n",
      "Classification: Uninformative\n",
      "\n",
      "Headline: 8 Stocks To Watch For December 17, 2019\n",
      "Classification: Uninformative\n",
      "\n",
      "Headline: Mid-Morning Market Update: Markets Edge Higher; Jabil Beats Q1 Expectations\n",
      "Classification: Informative\n",
      "\n",
      "Headline: 30 Stocks Moving in Tuesday's Pre-Market Session\n",
      "Classification: Uninformative\n",
      "\n",
      "Headline: 9 Energy Stocks Moving In Wednesday's Pre-Market Session\n",
      "Classification: Uninformative\n",
      "\n",
      "Kept 63544 headlines, removed 503 headlines\n",
      "\n",
      "First 20 kept headlines: \n",
      "0     Atlantic American Q3 EPS $(0.07) Down From $0....\n",
      "1     Atlantic American Q1 EPS $0.19 Up From $(0.25)...\n",
      "2     Atlantic American Q4 EPS $0.01 Down From $0.12...\n",
      "3     Atlantic American Q3 EPS $0.04 Up From $0.03 Y...\n",
      "4     Applied Optoelectronics to Participate in Raym...\n",
      "5     Applied Optoelectronics Amended S-3 Shows Regi...\n",
      "6     Piper Jaffray Maintains Neutral on Applied Opt...\n",
      "7     Applied Optoelectronics Sees Q4 Adj. EPS $(0.2...\n",
      "8     Applied Optoelectronics Q3 Adj. EPS $(0.15) Be...\n",
      "9     Rosenblatt Upgrades Applied Optoelectronics to...\n",
      "10    Applied Optoelectronics Files for Up to $250M ...\n",
      "11    JP Morgan Downgrades Applied Optoelectronics t...\n",
      "12    Applied Optoelectronics shares are trading hig...\n",
      "13    Applied Optoelectronics Q2 EPS $(0.26) Beats $...\n",
      "14     A Preview Of Applied Optoelectronics Q2 Earnings\n",
      "15    JP Morgan Initiates Coverage On Applied Optoel...\n",
      "16    Applied Optoelectronics shares are trading hig...\n",
      "17    Rosenblatt Upgrades Applied Optoelectronics to...\n",
      "18    Applied Optoelectronics Posts Q1 Earnings Miss...\n",
      "19    Applied Optoelectronics shares are trading low...\n",
      "Name: title, dtype: object\n",
      "\n",
      "First 20 removed headlines: \n",
      "501     35 Healthcare Stocks Moving In Wednesday's Aft...\n",
      "1368                  New 52-Week Lows For Monday Morning\n",
      "1676    Stocks That Managed To Breach 52-Week Highs We...\n",
      "2074         Companies That Achieved 52-Week Highs Friday\n",
      "2202    13 Industrials Stocks Moving In Thursday's Aft...\n",
      "3460              Earnings Scheduled For October 15, 2019\n",
      "3466    10 Stocks Taking A Big Hit On Mexico Tariff Th...\n",
      "3952    Antero Resources shares are trading higher aft...\n",
      "4552    Benzinga's Top Upgrades, Downgrades For Octobe...\n",
      "4622    35 Technology Stocks Moving In Thursday's Afte...\n",
      "6963    Benzinga's Top Upgrades, Downgrades For Septem...\n",
      "7154    Benzinga's Top Upgrades, Downgrades For Octobe...\n",
      "7158    Benzinga's Top Upgrades, Downgrades For Septem...\n",
      "7167    Benzinga's Top Upgrades, Downgrades For April ...\n",
      "7174    Benzinga's Top Upgrades, Downgrades For Januar...\n",
      "7289                     28 Biggest Movers From Yesterday\n",
      "7294                     50 Biggest Movers From Yesterday\n",
      "7295                     58 Biggest Movers From Yesterday\n",
      "7298                     66 Biggest Movers From Yesterday\n",
      "7299                     58 Biggest Movers From Yesterday\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def is_uninformative_headline(headline):\n",
    "    patterns = [\n",
    "        r'^\\d+\\s+(Stocks|Companies|Biggest|Largest)',  # \"74 Biggest Movers\", \"10 Stocks\"\n",
    "        r'^\\d+\\s+Stocks (Moving|To Watch)',  # \"Stocks Moving\", \"Stocks To Watch\"\n",
    "        r'Stocks That Hit',  # \"Stocks That Hit 52-Week Highs\"\n",
    "        r'(Top|Biggest) (Upgrades|Downgrades)',  # Benzinga's Top Upgrades/Downgrades\n",
    "        r'Earnings Scheduled For',  # Earnings calendar entries\n",
    "        r'Pre-Market Session$',  # Pre-market movement summaries\n",
    "        r'Price Target Changes',  # Price target updates without specifics\n",
    "        r'^\\d+.*Moving In',  # \"7 Technology Stocks Moving In\"\n",
    "        r'52-Week (Highs|Lows)',  # 52-week high/low lists\n",
    "    ]\n",
    "    \n",
    "    # Check if headline matches any of the patterns\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, headline, re.IGNORECASE):\n",
    "            # Additional check: if the headline contains \":\" followed by a company name,\n",
    "            # it might still be informative\n",
    "            if \":\" in headline:\n",
    "                return False\n",
    "            return True\n",
    "            \n",
    "    return False\n",
    "\n",
    "def filter_headlines(df, headline_column='title'):\n",
    "    # Create mask of informative headlines\n",
    "    mask = ~df[headline_column].apply(is_uninformative_headline)\n",
    "    \n",
    "    # Split into informative and uninformative headlines\n",
    "    informative_headlines = df[mask]\n",
    "    removed_headlines = df[~mask]\n",
    "    \n",
    "    return informative_headlines, removed_headlines\n",
    "\n",
    "# Test the function with sample headlines\n",
    "test_headlines = [\n",
    "    \"74 Biggest Movers From Yesterday\",\n",
    "    \"8 Stocks To Watch For December 17, 2019\",\n",
    "    \"Mid-Morning Market Update: Markets Edge Higher; Jabil Beats Q1 Expectations\",\n",
    "    \"30 Stocks Moving in Tuesday's Pre-Market Session\",\n",
    "    \"9 Energy Stocks Moving In Wednesday's Pre-Market Session\"\n",
    "]\n",
    "\n",
    "print(\"Sample classifications:\")\n",
    "print()\n",
    "for headline in test_headlines:\n",
    "    result = \"Uninformative\" if is_uninformative_headline(headline) else \"Informative\"\n",
    "    print(f\"\\nHeadline: {headline}\")\n",
    "    print(f\"Classification: {result}\")\n",
    "    \n",
    "\n",
    "input_path = \"output2/filtered_data5.csv\"\n",
    "output_path = \"output2/filtered_data6.csv\"    \n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "informative_df, removed_df = filter_headlines(df)\n",
    "print()\n",
    "print(f\"Kept {len(informative_df)} headlines, removed {len(removed_df)} headlines\")\n",
    "print()\n",
    "print(f\"First 20 kept headlines: \")\n",
    "print(informative_df['title'].head(20))\n",
    "print()\n",
    "print(f\"First 20 removed headlines: \")\n",
    "print(removed_df['title'].head(20))\n",
    "\n",
    "informative_df.to_csv(output_path)\n",
    "removed_df.to_csv('output2/removed_irrelevant_headlines.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7af6e-d1a4-4486-9498-95908fe0ef3b",
   "metadata": {},
   "source": [
    "## Remove Similar Headlines\n",
    "Use Damerau-Levenshtein similarity to remove nearly identical headlines about the same stock on the same day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2142817-d32c-4879-a3a9-c3f489a7fb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63544 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparing headlines: 100%|██████████████████████████████████████████████████████████████| 47500/47500 [04:53<00:00, 161.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total headlines removed: 2451\n",
      "\n",
      "Example similar headlines removed:\n",
      "\n",
      "Stock: AAOI - Date: 2019-11-06 00:00:00\n",
      "Similarity: 0.61\n",
      "1: Applied Optoelectronics Sees Q4 Adj. EPS $(0.23)-$(0.21) vs $(0.18) Est., Sales $46M-$49M vs $51.84M Est.\n",
      "2: Applied Optoelectronics Q3 Adj. EPS $(0.15) Beats $(0.24) Estimate, Sales $46.1M Miss $47.75M Estimate\n",
      "\n",
      "Stock: AAON - Date: 2019-10-31 00:00:00\n",
      "Similarity: 0.813\n",
      "1: AAON Q3 EPS $0.26 Down From $0.27 YoY, Sales $113.5M Up From $112.937M YoY\n",
      "2: AAON Earlier Reported Q3 EPS $0.26 Down From $0.27 YoY, Sales $113.5M Up From $112.937M YoY\n",
      "\n",
      "Stock: AAP - Date: 2018-11-14 00:00:00\n",
      "Similarity: 0.744\n",
      "1: Deutsche Bank Maintains Buy on Advance Auto Parts, Raises Price Target to $205\n",
      "2: Citigroup Maintains Neutral on Advance Auto Parts, Raises Price Target to $185\n",
      "\n",
      "Stock: AAP - Date: 2018-11-14 00:00:00\n",
      "Similarity: 0.833\n",
      "1: Deutsche Bank Maintains Buy on Advance Auto Parts, Raises Price Target to $205\n",
      "2: UBS Maintains Buy on Advance Auto Parts, Raises Price Target to $215\n",
      "\n",
      "Stock: AAP - Date: 2018-11-14 00:00:00\n",
      "Similarity: 0.718\n",
      "1: Deutsche Bank Maintains Buy on Advance Auto Parts, Raises Price Target to $205\n",
      "2: Credit Suisse Maintains Outperform on Advance Auto Parts, Raises Price Target to $195\n",
      "Saved 61093 rows to output2/filtered_data7.csv\n"
     ]
    }
   ],
   "source": [
    "def remove_similar_headlines(df):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    groups = df.groupby(['stock', 'date'])\n",
    "    rows_to_drop = set()\n",
    "    similar_pairs = []\n",
    "    \n",
    "    # Get total number of comparisons for progress bar\n",
    "    total_comparisons = sum(len(list(combinations(group.index, 2))) \n",
    "                          for _, group in groups if len(group) > 1)\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=total_comparisons, desc=\"Comparing headlines\")\n",
    "    current_comparisons = 0\n",
    "    \n",
    "    for (stock, date), group in groups:\n",
    "        if len(group) <= 1:\n",
    "            continue\n",
    "            \n",
    "        for idx1, idx2 in combinations(group.index, 2):\n",
    "            headline1 = group.loc[idx1, 'title']\n",
    "            headline2 = group.loc[idx2, 'title']\n",
    "            \n",
    "            similarity = damerau_levenshtein.normalized_similarity(headline1, headline2)\n",
    "            \n",
    "            if similarity > 0.6:\n",
    "                rows_to_drop.add(idx2)\n",
    "                similar_pairs.append({\n",
    "                    'stock': stock,\n",
    "                    'date': date,\n",
    "                    'headline1': headline1,\n",
    "                    'headline2': headline2,\n",
    "                    'similarity': round(similarity, 3)\n",
    "                })\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    print(f\"\\nTotal headlines removed: {len(rows_to_drop)}\")\n",
    "    print(\"\\nExample similar headlines removed:\")\n",
    "    for pair in similar_pairs[:5]:\n",
    "        print(f\"\\nStock: {pair['stock']} - Date: {pair['date']}\")\n",
    "        print(f\"Similarity: {pair['similarity']}\")\n",
    "        print(f\"1: {pair['headline1']}\")\n",
    "        print(f\"2: {pair['headline2']}\")\n",
    "    \n",
    "    return df.drop(index=rows_to_drop).reset_index(drop=True)\n",
    "\n",
    "input_path = \"output2/filtered_data6.csv\"\n",
    "output_path = \"output2/filtered_data7.csv\" \n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "deduped_df = remove_similar_headlines(df)\n",
    "\n",
    "deduped_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(deduped_df)} rows to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44286f7b-b735-41ce-900a-d21d89e8170a",
   "metadata": {},
   "source": [
    "## Filter for Valid Trading Sessions after Headline\n",
    "Filter headlines on whether they can be associated with a valid subsequent trading session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94581974-8a3e-4b3a-a6ce-939f056b49ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original headlines: 61093\n",
      "Filtered headlines: 60421\n"
     ]
    }
   ],
   "source": [
    "def filter_headlines_by_trading_session(df):\n",
    "    \"\"\"\n",
    "    Filter headlines based on trading session rules:\n",
    "    - Before 6 AM: Same day trading (open to close)\n",
    "    - 6 AM to 4 PM: Same day close to next day close\n",
    "    - After 4 PM: Next day open to next day close\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert date column to datetime if it's not already\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "    #df['date'] = df['date'].dt.tz_convert('America/New_York')\n",
    "    #df['date'] = df['date'].dt.tz_localize('America/New_York') \n",
    "    \n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df_filtered = df.copy()\n",
    "    \n",
    "    # Add helper columns for filtering\n",
    "    df_filtered['day_of_week'] = df_filtered['date'].dt.dayofweek  # Monday = 0, Sunday = 6\n",
    "    df_filtered['hour'] = df_filtered['time'].str.split(':').str[0]\n",
    "    df_filtered['hour'] = df_filtered['hour'].astype(int)\n",
    "    df_filtered['minute'] = df_filtered['time'].str.split(':').str[1]\n",
    "    df_filtered['minute'] = df_filtered['minute'].astype(int)\n",
    "    \n",
    "    # Filter conditions\n",
    "    def has_valid_trading_session(row):\n",
    "        # Skip weekends\n",
    "        if row['day_of_week'] in [5, 6]:\n",
    "            return False\n",
    "            \n",
    "        # Before 9:30 AM - needs same day trading\n",
    "        if row['hour'] < 9 or (row['hour'] == 9 and row['minute'] < 30):\n",
    "            return True  \n",
    "            \n",
    "        # Between 9:30 AM and 4 PM - needs next trading day\n",
    "        elif (row['hour'] == 9 and row['minute'] >= 30) or (row['hour'] >= 10 and row['hour'] < 16):\n",
    "            next_date = row['date'] + timedelta(days=1)\n",
    "            while next_date.dayofweek in [5, 6]:  # Skip weekends\n",
    "                next_date += timedelta(days=1)\n",
    "            return True  \n",
    "            \n",
    "        # After 4 PM - needs next trading day\n",
    "        else:  # hour >= 16\n",
    "            next_date = row['date'] + timedelta(days=1)\n",
    "            while next_date.dayofweek in [5, 6]:  # Skip weekends\n",
    "                next_date += timedelta(days=1)\n",
    "            return True  \n",
    "    \n",
    "    # Apply filtering\n",
    "    mask = df_filtered.apply(has_valid_trading_session, axis=1)\n",
    "    df_filtered = df_filtered[mask]\n",
    "    \n",
    "    # Drop helper columns\n",
    "    df_filtered = df_filtered.drop(['day_of_week', 'hour'], axis=1)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "input_path = 'output2/filtered_data7.csv'\n",
    "output_path = 'output2/filtered_data8.csv'\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "filtered_df = filter_headlines_by_trading_session(df)\n",
    "print(f\"Original headlines: {len(df)}\")\n",
    "print(f\"Filtered headlines: {len(filtered_df)}\")\n",
    "filtered_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88869b9-1ba1-48e7-8459-b771f299c6ab",
   "metadata": {},
   "source": [
    "## Calculate Trading Returns\n",
    "Retrieve stock price data then calculate returns for the trading session following each headline:\n",
    "\n",
    "Pre-market headlines: same-day open to close\n",
    "During market: same-day close to next-day close\n",
    "After-market: next-day open to close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb08962-1657-44fb-aacc-f5204365d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 stocks not found in cache, attempting to fetch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b1d4ad51bf4d4a9f49cafd447ebfd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$ARRY: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$BAM: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$BEAT: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$BTX: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$CR: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$CRC: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$CUB: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$DBD: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$DNB: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$EDR: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$EE: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$GIG: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$GPOR: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$HTZ: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$ICON: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$LGCY: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$LTM: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$NE: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$SERV: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$STI: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n",
      "$VAL: possibly delisted; no price data found  (1d 2018-10-01 -> 2020-01-05) (Yahoo error = \"Data doesn't exist for startDate = 1538366400, endDate = 1578200400\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing headlines...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_trading_returns(headlines_df, cache_dir='price_cache', batch_size=50, start_date=None, end_date=None):\n",
    "    \n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    headlines_df['date'] = pd.to_datetime(headlines_df['date'])\n",
    "    \n",
    "    if not start_date:\n",
    "        start_date = headlines_df['date'].min().strftime('%Y-%m-%d')\n",
    "    if not end_date:\n",
    "        end_date = (headlines_df['date'].max() + timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    result_df = headlines_df.copy()    \n",
    "    unique_stocks = list(headlines_df['stock'].unique())\n",
    "    \n",
    "    # Load existing cache manifest if it exists\n",
    "    manifest_path = os.path.join(cache_dir, 'manifest.json')\n",
    "    if os.path.exists(manifest_path):\n",
    "        with open(manifest_path, 'r') as f:\n",
    "            processed_stocks = set(json.load(f))\n",
    "    else:\n",
    "        processed_stocks = set()\n",
    "    \n",
    "    # Get stocks that still need processing\n",
    "    stocks_to_process = [s for s in unique_stocks if s not in processed_stocks]\n",
    "    \n",
    "    price_cache = {}\n",
    "    \n",
    "    # Load existing cache files\n",
    "    for stock in processed_stocks:\n",
    "        cache_file = os.path.join(cache_dir, f\"{stock}.csv\")\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                price_cache[stock] = pd.read_csv(cache_file)\n",
    "                price_cache[stock]['Date'] = pd.to_datetime(price_cache[stock]['Date'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache for {stock}: {str(e)}\")\n",
    "                if stock in processed_stocks:\n",
    "                    processed_stocks.remove(stock)\n",
    "    \n",
    "    # Fetch price data for remaining stocks with progress bar\n",
    "    print(f\"{len(stocks_to_process)} stocks not found in cache, attempting to fetch...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(stocks_to_process), batch_size)):\n",
    "        batch = stocks_to_process[i:i+batch_size]\n",
    "        \n",
    "        for stock in batch:\n",
    "            try:\n",
    "                # Add delay between requests\n",
    "                time.sleep(0.5)  # 500ms delay\n",
    "                \n",
    "                # Get data from yfinance\n",
    "                ticker = yf.Ticker(stock)\n",
    "                hist = ticker.history(start=start_date, end=end_date)\n",
    "                \n",
    "                if not hist.empty:\n",
    "                    # Reset index to make date a column\n",
    "                    hist = hist.reset_index()\n",
    "                    hist['Date'] = pd.to_datetime(hist['Date'])\n",
    "                    price_data = hist[['Date', 'Open', 'Close']].copy()\n",
    "                    \n",
    "                    # Save to cache file\n",
    "                    cache_file = os.path.join(cache_dir, f\"{stock}.csv\")\n",
    "                    price_data.to_csv(cache_file, index=False)\n",
    "                    \n",
    "                    # Store in memory cache\n",
    "                    price_cache[stock] = price_data\n",
    "                    processed_stocks.add(stock)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {stock}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save progress after each batch\n",
    "        with open(manifest_path, 'w') as f:\n",
    "            json.dump(list(processed_stocks), f)\n",
    "    \n",
    "    def get_trading_prices(row):\n",
    "        headline_date = pd.to_datetime(row['datetime'])\n",
    "        headline_hour = headline_date.hour\n",
    "        headline_minute = headline_date.minute\n",
    "        headline_day = headline_date.date()\n",
    "        stock = row['stock']\n",
    "        \n",
    "        def get_next_trading_day(current_date):\n",
    "            next_date = current_date + timedelta(days=1)\n",
    "            while next_date.weekday() in [5, 6]:  # Skip weekends\n",
    "                next_date += timedelta(days=1)\n",
    "            return next_date\n",
    "        \n",
    "        if stock not in price_cache:\n",
    "            return {\n",
    "                'entry_price': None,\n",
    "                'exit_price': None\n",
    "            }\n",
    "        \n",
    "        stock_prices = price_cache[stock]\n",
    "        \n",
    "        # Before 9:30 AM: Same day open to same day close\n",
    "        if headline_hour < 9 or (headline_hour == 9 and headline_minute < 30):\n",
    "            same_day_prices = stock_prices[stock_prices['Date'].dt.date == headline_day]\n",
    "            if len(same_day_prices) > 0:\n",
    "                return {\n",
    "                    'entry_price': same_day_prices.iloc[0]['Open'],\n",
    "                    'exit_price': same_day_prices.iloc[0]['Close']\n",
    "                }\n",
    "        \n",
    "        # 9:30 AM to 4 PM: Same day close to next trading day close\n",
    "        elif (headline_hour == 9 and headline_minute >= 30) or (headline_hour >= 10 and headline_hour < 16):\n",
    "            same_day_prices = stock_prices[stock_prices['Date'].dt.date == headline_day]\n",
    "            next_trading_day = get_next_trading_day(headline_day)\n",
    "            next_day_prices = stock_prices[stock_prices['Date'].dt.date == next_trading_day]\n",
    "            \n",
    "            if len(same_day_prices) > 0 and len(next_day_prices) > 0:\n",
    "                return {\n",
    "                    'entry_price': same_day_prices.iloc[0]['Close'],\n",
    "                    'exit_price': next_day_prices.iloc[0]['Close']\n",
    "                }\n",
    "        \n",
    "        # After 4 PM: Next trading day open to close\n",
    "        else:  # headline_hour >= 16\n",
    "            next_trading_day = get_next_trading_day(headline_day)\n",
    "            next_day_prices = stock_prices[stock_prices['Date'].dt.date == next_trading_day]\n",
    "            \n",
    "            if len(next_day_prices) > 0:\n",
    "                return {\n",
    "                    'entry_price': next_day_prices.iloc[0]['Open'],\n",
    "                    'exit_price': next_day_prices.iloc[0]['Close']\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'entry_price': None,\n",
    "            'exit_price': None\n",
    "        }\n",
    "    \n",
    "    print(\"Processing headlines...\")\n",
    "    # Apply the price lookup to each headline\n",
    "    prices_data = result_df.apply(get_trading_prices, axis=1)\n",
    "    \n",
    "    # Add the new columns\n",
    "    result_df['entry_price'] = prices_data.apply(lambda x: x['entry_price'])\n",
    "    result_df['exit_price'] = prices_data.apply(lambda x: x['exit_price'])\n",
    "    \n",
    "    # Calculate returns\n",
    "    result_df['return_pct'] = (\n",
    "        (result_df['exit_price'] - result_df['entry_price']) / \n",
    "        result_df['entry_price'] * 100\n",
    "    ).round(2)\n",
    "    print(\"Finished!\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "input_path = 'output2/filtered_data8.csv'\n",
    "output_path = 'output2/filtered_data9.csv'\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "filtered_df = filter_headlines_by_trading_session(df)\n",
    "result_df = get_trading_returns(filtered_df, batch_size=50)  \n",
    "valid_trades_df = result_df.dropna(subset=['return_pct'])\n",
    "valid_trades_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0f22b-fea1-4b31-a971-360a9e0ad1c7",
   "metadata": {},
   "source": [
    "## Stratified Sampling\n",
    "Take a representative sample of the dataset using stratified sampling to maintain seasonal distribution, while reducing the data size for LLM processing.\n",
    "\n",
    "Performing sampling early in the pipeline would have greatly sped up some of the above operations. However, it was done last so that different samples could easily be drawn from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a370acb-bb69-4f21-a175-dcbed29a772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique stocks in dataset: 2090\n",
      "Total sampled stock-month combinations: 895\n",
      "Original headlines: 59257\n",
      "Sampled headlines: 2995\n",
      "Original trading days: 324\n",
      "Sampled trading days: 312\n",
      "\n",
      "Original monthly distribution:\n",
      "date\n",
      "2018-10    4621\n",
      "2018-11    3713\n",
      "2018-12    2541\n",
      "2019-01    3694\n",
      "2019-02    3940\n",
      "2019-03    3008\n",
      "2019-04    3916\n",
      "2019-05    4532\n",
      "2019-06    2842\n",
      "2019-07    4650\n",
      "2019-08    4257\n",
      "2019-09    3879\n",
      "2019-10    5737\n",
      "2019-11    4493\n",
      "2019-12    3434\n",
      "Freq: M, dtype: int64\n",
      "\n",
      "Sampled monthly distribution:\n",
      "date\n",
      "2018-10    182\n",
      "2018-11    174\n",
      "2018-12    120\n",
      "2019-01    263\n",
      "2019-02    179\n",
      "2019-03    158\n",
      "2019-04    154\n",
      "2019-05    266\n",
      "2019-06    160\n",
      "2019-07    214\n",
      "2019-08    205\n",
      "2019-09    210\n",
      "2019-10    338\n",
      "2019-11    213\n",
      "2019-12    159\n",
      "Freq: M, dtype: int64\n",
      "\n",
      "Top 10 stocks in original data:\n",
      "stock\n",
      "GOOGL    500\n",
      "NFLX     482\n",
      "TSLA     468\n",
      "GOOG     366\n",
      "PCG      328\n",
      "BABA     310\n",
      "MU       291\n",
      "QCOM     266\n",
      "NVDA     261\n",
      "MRK      259\n",
      "dtype: int64\n",
      "\n",
      "Top 10 stocks in sampled data:\n",
      "stock\n",
      "PCG      77\n",
      "NFLX     75\n",
      "EA       43\n",
      "GOOGL    41\n",
      "CVS      34\n",
      "KR       30\n",
      "GILD     27\n",
      "NVDA     27\n",
      "NVO      26\n",
      "GME      23\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def stratified_stock_sample(df, fraction):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Create month-year column for stratification\n",
    "    df['month_year'] = df['date'].dt.to_period('M')\n",
    "    \n",
    "    # Get all unique stocks and their corresponding month-years\n",
    "    stock_info = df[['stock', 'month_year']].drop_duplicates()\n",
    "    \n",
    "    # Group by stock to get unique stocks\n",
    "    unique_stocks = df['stock'].unique()\n",
    "    print(f\"Total unique stocks in dataset: {len(unique_stocks)}\")\n",
    "    \n",
    "    # Group the unique stock-month combinations by month-year\n",
    "    grouped_by_month = stock_info.groupby('month_year')\n",
    "    \n",
    "    # Initialize an empty list to store sampled stock-month combinations\n",
    "    sampled_stock_months = []\n",
    "    \n",
    "    # For each month-year, sample a proportional number of stocks\n",
    "    for month, group in grouped_by_month:\n",
    "        # Calculate how many stocks to sample from this month\n",
    "        n_to_sample = max(1, int(len(group) * fraction))\n",
    "        \n",
    "        # Sample stocks from this month\n",
    "        month_sample = group.sample(n=n_to_sample, random_state=42)\n",
    "        \n",
    "        # Add to our list of sampled stock-month combinations\n",
    "        sampled_stock_months.append(month_sample)\n",
    "    \n",
    "    # Combine all the sampled stock-month combinations\n",
    "    sampled_stock_months_df = pd.concat(sampled_stock_months)\n",
    "    \n",
    "    print(f\"Total sampled stock-month combinations: {len(sampled_stock_months_df)}\")\n",
    "    \n",
    "    # Return the dataframe rows corresponding to the sampled stock-month combinations\n",
    "    sampled_df = df.merge(sampled_stock_months_df, on=['stock', 'month_year'])\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Sampling rate\n",
    "sample_fraction = 0.05\n",
    "input_path = 'output2/filtered_data9.csv'\n",
    "output_path = 'output2/filtered_data10.csv'\n",
    "df = pd.read_csv(input_path)\n",
    "sampled_df = stratified_stock_sample(df, sample_fraction)\n",
    "\n",
    "# Show the results\n",
    "print(f\"Original headlines: {len(df)}\")\n",
    "print(f\"Sampled headlines: {len(sampled_df)}\")\n",
    "\n",
    "print(f\"Original trading days: {df['date'].nunique()}\")\n",
    "print(f\"Sampled trading days: {sampled_df['date'].nunique()}\")\n",
    "\n",
    "\n",
    "# Verify the month distribution in original vs sampled data\n",
    "original_monthly_dist = df.groupby(pd.to_datetime(df['date']).dt.to_period('M')).size()\n",
    "sampled_monthly_dist = sampled_df.groupby(sampled_df['date'].dt.to_period('M')).size()\n",
    "\n",
    "# Compare distributions\n",
    "print(\"\\nOriginal monthly distribution:\")\n",
    "print(original_monthly_dist)\n",
    "print(\"\\nSampled monthly distribution:\")\n",
    "print(sampled_monthly_dist)\n",
    "\n",
    "# Verify stock distribution\n",
    "original_stock_dist = df.groupby('stock').size().sort_values(ascending=False).head(10)\n",
    "sampled_stock_dist = sampled_df.groupby('stock').size().sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 stocks in original data:\")\n",
    "print(original_stock_dist)\n",
    "print(\"\\nTop 10 stocks in sampled data:\")\n",
    "print(sampled_stock_dist)\n",
    "\n",
    "# Save results\n",
    "sampled_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e2f4d-3306-4b45-a3d8-8ac59e1f76eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
